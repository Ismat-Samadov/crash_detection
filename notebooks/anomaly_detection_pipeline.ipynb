{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Anomaly Detection Pipeline\n",
    "## Natural Gas Pipeline Operations Monitoring\n",
    "\n",
    "**Purpose**: Detect operational anomalies in gas pipeline sensor data from Mardakan, Sumqayit, and Turkan stations.\n",
    "\n",
    "**Workflow**:\n",
    "1. Data Loading & Validation\n",
    "2. Exploratory Data Analysis\n",
    "3. Preprocessing & Feature Engineering\n",
    "4. Model Training & Benchmarking (Multiple Algorithms)\n",
    "5. Model Evaluation & Selection\n",
    "6. Model Persistence & Results Export\n",
    "\n",
    "**Directory Structure**:\n",
    "- `/data` - Raw CSV files\n",
    "- `/charts` - Visualizations\n",
    "- `/outputs` - Metrics, tables, summaries\n",
    "- `/artifacts` - Trained models, scalers, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Sklearn - Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sklearn - Anomaly Detection Models\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "# Sklearn - Metrics\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Time tracking\n",
    "import time\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# CONFIGURATION\n",
    "# ========================\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Directory paths (relative to notebook location)\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CHARTS_DIR = BASE_DIR / \"charts\"\n",
    "OUTPUTS_DIR = BASE_DIR / \"outputs\"\n",
    "ARTIFACTS_DIR = BASE_DIR / \"artifacts\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [CHARTS_DIR, OUTPUTS_DIR, ARTIFACTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data files\n",
    "DATA_FILES = [\n",
    "    DATA_DIR / \"Mardakan.csv\",\n",
    "    DATA_DIR / \"Sumqayit.csv\",\n",
    "    DATA_DIR / \"Turkan.csv\",\n",
    "]\n",
    "\n",
    "# Column mapping (Azerbaijani to English)\n",
    "COLUMN_RENAME_MAP = {\n",
    "    \"TARƒ∞X\": \"timestamp\",\n",
    "    \"X√úSUSƒ∞ √á∆èKƒ∞\\n(kq/m3)\": \"density_kg_m3\",\n",
    "    \"T∆èZYƒ∞QL∆èR\\nF∆èRQƒ∞ (kPa)\": \"pressure_diff_kpa\",\n",
    "    \"T∆èZYƒ∞Q (kPa)\": \"pressure_kpa\",\n",
    "    \"TEMPERATUR\\n(C)\": \"temperature_c\",\n",
    "    \"SAATLIQ\\nS∆èRF(min m3)\": \"hourly_flow_m3\",\n",
    "    \"S∆èRF (min m3)\": \"total_flow_m3\",\n",
    "}\n",
    "\n",
    "# Feature configurations\n",
    "SENSOR_FEATURES = [\n",
    "    \"density_kg_m3\",\n",
    "    \"pressure_diff_kpa\",\n",
    "    \"pressure_kpa\",\n",
    "    \"temperature_c\",\n",
    "    \"hourly_flow_m3\",\n",
    "    \"total_flow_m3\",\n",
    "]\n",
    "\n",
    "TEMPORAL_FEATURES = [\"hour\", \"day_of_week\", \"month\", \"year\"]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\"location\"]\n",
    "\n",
    "# Model hyperparameters\n",
    "CONTAMINATION_RATE = 0.01  # Expected anomaly rate (1%)\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.4f}')\n",
    "\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Random Seed: {RANDOM_SEED}\")\n",
    "print(f\"Contamination Rate: {CONTAMINATION_RATE * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tag_csv(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single CSV file and add location metadata.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with renamed columns and location tag\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Rename columns BEFORE concat\n",
    "    df = df.rename(columns=COLUMN_RENAME_MAP)\n",
    "    \n",
    "    # Add location column\n",
    "    df = df.copy()\n",
    "    df[\"location\"] = file_path.stem\n",
    "    \n",
    "    print(f\"‚úì Loaded {file_path.name}: {len(df):,} rows\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def combine_location_files(file_paths: List[Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine multiple location CSV files into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of paths to CSV files\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with validation\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    total_expected_rows = 0\n",
    "    \n",
    "    for path in file_paths:\n",
    "        df = load_and_tag_csv(path)\n",
    "        dataframes.append(df)\n",
    "        total_expected_rows += len(df)\n",
    "    \n",
    "    combined_df = pd.concat(\n",
    "        dataframes,\n",
    "        axis=0,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Expected total rows : {total_expected_rows:,}\")\n",
    "    print(f\"Combined DF rows    : {len(combined_df):,}\")\n",
    "    print(\n",
    "        f\"Row loss detected   : {'YES ‚ö†Ô∏è' if total_expected_rows != len(combined_df) else 'NO ‚úì'}\"\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    required_columns = set(COLUMN_RENAME_MAP.values()) | {\"location\"}\n",
    "    missing = required_columns - set(combined_df.columns)\n",
    "    \n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing expected columns: {missing}\")\n",
    "    \n",
    "    print(f\"Schema validation   : PASSED ‚úì\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data files...\\n\")\n",
    "df_raw = combine_location_files(DATA_FILES)\n",
    "\n",
    "print(f\"\\nFinal columns: {df_raw.columns.tolist()}\")\n",
    "print(f\"Unique locations: {df_raw['location'].unique().tolist()}\")\n",
    "print(f\"\\nDataFrame shape: {df_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality report.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Quality report DataFrame\n",
    "    \"\"\"\n",
    "    quality_report = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'non_null_count': df.count(),\n",
    "        'null_count': df.isnull().sum(),\n",
    "        'null_pct': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'unique_values': df.nunique(),\n",
    "        'unique_pct': (df.nunique() / len(df) * 100).round(2),\n",
    "    })\n",
    "    \n",
    "    return quality_report\n",
    "\n",
    "\n",
    "# Generate quality report\n",
    "quality_report = assess_data_quality(df_raw)\n",
    "print(\"\\nüìä DATA QUALITY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(quality_report)\n",
    "\n",
    "# Save to outputs\n",
    "quality_report.to_csv(OUTPUTS_DIR / \"01_data_quality_report_raw.csv\")\n",
    "print(f\"\\n‚úì Saved quality report to: {OUTPUTS_DIR / '01_data_quality_report_raw.csv'}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_nulls = df_raw.isnull().sum().sum()\n",
    "rows_with_nulls = df_raw.isnull().any(axis=1).sum()\n",
    "\n",
    "print(f\"\\nüìà Summary:\")\n",
    "print(f\"Total null values    : {total_nulls:,}\")\n",
    "print(f\"Rows with nulls      : {rows_with_nulls:,} ({rows_with_nulls/len(df_raw)*100:.2f}%)\")\n",
    "print(f\"Complete rows        : {len(df_raw) - rows_with_nulls:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Remove Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any null values\n",
    "df_clean = df_raw.dropna()\n",
    "\n",
    "print(f\"Rows before cleaning: {len(df_raw):,}\")\n",
    "print(f\"Rows after cleaning : {len(df_clean):,}\")\n",
    "print(f\"Rows removed        : {len(df_raw) - len(df_clean):,} ({(len(df_raw) - len(df_clean))/len(df_raw)*100:.2f}%)\")\n",
    "print(f\"\\nNull values remaining: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Verify no nulls remain\n",
    "assert df_clean.isnull().sum().sum() == 0, \"Null values still present after cleaning!\"\n",
    "print(\"‚úì Data cleaning validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"üìä DESCRIPTIVE STATISTICS\\n\")\n",
    "stats = df_clean[SENSOR_FEATURES].describe()\n",
    "print(stats)\n",
    "\n",
    "# Save statistics\n",
    "stats.to_csv(OUTPUTS_DIR / \"02_descriptive_statistics.csv\")\n",
    "print(f\"\\n‚úì Saved statistics to: {OUTPUTS_DIR / '02_descriptive_statistics.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by location\n",
    "location_dist = df_clean['location'].value_counts()\n",
    "print(\"\\nüìç DISTRIBUTION BY LOCATION\")\n",
    "print(location_dist)\n",
    "\n",
    "# Time range\n",
    "print(f\"\\nüìÖ TIME RANGE\")\n",
    "print(f\"Start: {df_clean['timestamp'].min()}\")\n",
    "print(f\"End  : {df_clean['timestamp'].max()}\")\n",
    "print(f\"Duration: {(df_clean['timestamp'].max() - df_clean['timestamp'].min()).days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "correlation_matrix = df_clean[SENSOR_FEATURES].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, ax=ax, cbar_kws={'label': 'Correlation'})\n",
    "ax.set_title('Sensor Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHARTS_DIR / \"01_correlation_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Saved correlation matrix to: {CHARTS_DIR / '01_correlation_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for sensor features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(SENSOR_FEATURES):\n",
    "    axes[idx].hist(df_clean[feature], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feature}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sensor Feature Distributions', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHARTS_DIR / \"02_feature_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Saved distributions to: {CHARTS_DIR / '02_feature_distributions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract temporal features from timestamp column.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with 'timestamp' column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional temporal features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['year'] = df['timestamp'].dt.year\n",
    "    \n",
    "    print(\"‚úì Temporal features extracted:\")\n",
    "    print(f\"  - hour (0-23)\")\n",
    "    print(f\"  - day_of_week (0-6, Monday=0)\")\n",
    "    print(f\"  - month (1-12)\")\n",
    "    print(f\"  - year\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract temporal features\n",
    "df_features = extract_temporal_features(df_clean)\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nDataFrame shape after feature engineering: {df_features.shape}\")\n",
    "print(f\"New columns: {[col for col in df_features.columns if col not in df_clean.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode location\n",
    "df_encoded = pd.get_dummies(df_features, columns=['location'], prefix='loc', drop_first=False)\n",
    "\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "print(f\"Location columns: {[col for col in df_encoded.columns if col.startswith('loc_')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting & Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "feature_columns = SENSOR_FEATURES + TEMPORAL_FEATURES + [col for col in df_encoded.columns if col.startswith('loc_')]\n",
    "X = df_encoded[feature_columns].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Total features: {len(feature_columns)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: Train (70%), Validation (10%), Test (20%)\n",
    "X_temp, X_test = train_test_split(X, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "X_train, X_val = train_test_split(X_temp, test_size=VAL_SIZE/(1-TEST_SIZE), random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Data split (random_state={RANDOM_SEED}):\")\n",
    "print(f\"  Train set: {X_train.shape[0]:>6,} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"  Val set  : {X_val.shape[0]:>6,} samples ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"  Test set : {X_test.shape[0]:>6,} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"  Total    : {X.shape[0]:>6,} samples\")\n",
    "\n",
    "# Verify no data leakage\n",
    "assert X_train.shape[0] + X_val.shape[0] + X_test.shape[0] == X.shape[0], \"Data split mismatch!\"\n",
    "print(\"\\n‚úì Data split validated (no leakage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaler on training data only (prevent data leakage)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì StandardScaler fitted on training data\")\n",
    "print(f\"  Mean   : {scaler.mean_[:3]} ... (showing first 3)\")\n",
    "print(f\"  Std Dev: {scaler.scale_[:3]} ... (showing first 3)\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = ARTIFACTS_DIR / \"scaler.joblib\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\\n‚úì Saved scaler to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development & Benchmarking\n",
    "\n",
    "We will train and compare the following anomaly detection algorithms:\n",
    "\n",
    "1. **Isolation Forest** - Tree-based ensemble method\n",
    "2. **Local Outlier Factor (LOF)** - Density-based local outlier detection\n",
    "3. **One-Class SVM** - Support vector-based boundary detection\n",
    "4. **DBSCAN** - Density-based clustering (noise points = anomalies)\n",
    "5. **Elliptic Envelope** - Robust covariance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name: str, X_train, X_val, \n",
    "                             fit_predict: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train anomaly detection model and evaluate performance.\n",
    "    \n",
    "    Args:\n",
    "        model: Sklearn anomaly detection model\n",
    "        model_name: Name of the model for reporting\n",
    "        X_train: Training data\n",
    "        X_val: Validation data\n",
    "        fit_predict: Whether model requires fit_predict (e.g., LOF, DBSCAN)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with model, predictions, metrics, and timing\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if fit_predict:\n",
    "        # Models like LOF and DBSCAN use fit_predict\n",
    "        y_train_pred = model.fit_predict(X_train)\n",
    "        y_val_pred = model.fit_predict(X_val)  # Re-fit on validation for these models\n",
    "    else:\n",
    "        # Standard fit/predict pattern\n",
    "        model.fit(X_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Convert predictions to binary (1 = normal, -1 = anomaly)\n",
    "    # Some models output 0/1, convert to -1/1\n",
    "    if set(np.unique(y_train_pred)).issubset({0, 1}):\n",
    "        y_train_pred = np.where(y_train_pred == 0, -1, 1)\n",
    "        y_val_pred = np.where(y_val_pred == 0, -1, 1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_anomalies = (y_train_pred == -1).sum()\n",
    "    train_anomaly_rate = train_anomalies / len(y_train_pred) * 100\n",
    "    \n",
    "    val_anomalies = (y_val_pred == -1).sum()\n",
    "    val_anomaly_rate = val_anomalies / len(y_val_pred) * 100\n",
    "    \n",
    "    print(f\"\\n‚úì Training completed in {train_time:.2f}s\")\n",
    "    print(f\"\\nTrain Set:\")\n",
    "    print(f\"  Normal    : {(y_train_pred == 1).sum():,} ({100 - train_anomaly_rate:.2f}%)\")\n",
    "    print(f\"  Anomalies : {train_anomalies:,} ({train_anomaly_rate:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nValidation Set:\")\n",
    "    print(f\"  Normal    : {(y_val_pred == 1).sum():,} ({100 - val_anomaly_rate:.2f}%)\")\n",
    "    print(f\"  Anomalies : {val_anomalies:,} ({val_anomaly_rate:.2f}%)\")\n",
    "    \n",
    "    # Try to get anomaly scores (if available)\n",
    "    scores_train = None\n",
    "    scores_val = None\n",
    "    \n",
    "    if hasattr(model, 'decision_function'):\n",
    "        try:\n",
    "            scores_train = model.decision_function(X_train)\n",
    "            scores_val = model.decision_function(X_val)\n",
    "            print(f\"\\nAnomaly Scores (validation):\")\n",
    "            print(f\"  Min  : {scores_val.min():.4f}\")\n",
    "            print(f\"  Max  : {scores_val.max():.4f}\")\n",
    "            print(f\"  Mean : {scores_val.mean():.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "    elif hasattr(model, 'score_samples'):\n",
    "        try:\n",
    "            scores_train = model.score_samples(X_train)\n",
    "            scores_val = model.score_samples(X_val)\n",
    "            print(f\"\\nAnomaly Scores (validation):\")\n",
    "            print(f\"  Min  : {scores_val.min():.4f}\")\n",
    "            print(f\"  Max  : {scores_val.max():.4f}\")\n",
    "            print(f\"  Mean : {scores_val.mean():.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_name': model_name,\n",
    "        'y_train_pred': y_train_pred,\n",
    "        'y_val_pred': y_val_pred,\n",
    "        'scores_train': scores_train,\n",
    "        'scores_val': scores_val,\n",
    "        'train_time': train_time,\n",
    "        'train_anomaly_rate': train_anomaly_rate,\n",
    "        'val_anomaly_rate': val_anomaly_rate,\n",
    "        'train_anomalies': train_anomalies,\n",
    "        'val_anomalies': val_anomalies,\n",
    "    }\n",
    "\n",
    "print(\"‚úì Training and evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=CONTAMINATION_RATE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_samples='auto',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "results_iso = train_and_evaluate_model(\n",
    "    iso_forest, \n",
    "    \"Isolation Forest\", \n",
    "    X_train_scaled, \n",
    "    X_val_scaled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=20,\n",
    "    contamination=CONTAMINATION_RATE,\n",
    "    novelty=False,  # For training data anomaly detection\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "results_lof = train_and_evaluate_model(\n",
    "    lof, \n",
    "    \"Local Outlier Factor\", \n",
    "    X_train_scaled, \n",
    "    X_val_scaled,\n",
    "    fit_predict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class_svm = OneClassSVM(\n",
    "    kernel='rbf',\n",
    "    gamma='auto',\n",
    "    nu=CONTAMINATION_RATE,  # nu is expected proportion of outliers\n",
    ")\n",
    "\n",
    "results_svm = train_and_evaluate_model(\n",
    "    one_class_svm, \n",
    "    \"One-Class SVM\", \n",
    "    X_train_scaled, \n",
    "    X_val_scaled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(\n",
    "    eps=0.5,\n",
    "    min_samples=50,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "results_dbscan = train_and_evaluate_model(\n",
    "    dbscan, \n",
    "    \"DBSCAN\", \n",
    "    X_train_scaled, \n",
    "    X_val_scaled,\n",
    "    fit_predict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Elliptic Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic = EllipticEnvelope(\n",
    "    contamination=CONTAMINATION_RATE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    support_fraction=None\n",
    ")\n",
    "\n",
    "results_elliptic = train_and_evaluate_model(\n",
    "    elliptic, \n",
    "    \"Elliptic Envelope\", \n",
    "    X_train_scaled, \n",
    "    X_val_scaled\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = [\n",
    "    results_iso,\n",
    "    results_lof,\n",
    "    results_svm,\n",
    "    results_dbscan,\n",
    "    results_elliptic,\n",
    "]\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for result in all_results:\n",
    "    comparison_data.append({\n",
    "        'Model': result['model_name'],\n",
    "        'Train Time (s)': round(result['train_time'], 3),\n",
    "        'Train Anomaly Rate (%)': round(result['train_anomaly_rate'], 2),\n",
    "        'Val Anomaly Rate (%)': round(result['val_anomaly_rate'], 2),\n",
    "        'Train Anomalies': result['train_anomalies'],\n",
    "        'Val Anomalies': result['val_anomalies'],\n",
    "        'Rate Consistency': abs(result['train_anomaly_rate'] - result['val_anomaly_rate']),\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Rate Consistency')\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MODEL COMPARISON & BENCHMARKING\")\n",
    "print(\"=\"*100)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nNote: 'Rate Consistency' = absolute difference between train and val anomaly rates\")\n",
    "print(\"      Lower values indicate more consistent behavior across datasets.\")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(OUTPUTS_DIR / \"03_model_comparison.csv\", index=False)\n",
    "print(f\"\\n‚úì Saved comparison to: {OUTPUTS_DIR / '03_model_comparison.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Model Comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Training time comparison\n",
    "ax1 = axes[0, 0]\n",
    "bars1 = ax1.barh(comparison_df['Model'], comparison_df['Train Time (s)'], color='steelblue')\n",
    "ax1.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "ax1.set_title('Training Time Comparison', fontweight='bold', fontsize=12)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}s', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Anomaly rate comparison\n",
    "ax2 = axes[0, 1]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "ax2.bar(x_pos - width/2, comparison_df['Train Anomaly Rate (%)'], width, \n",
    "        label='Train', color='coral', alpha=0.8)\n",
    "ax2.bar(x_pos + width/2, comparison_df['Val Anomaly Rate (%)'], width, \n",
    "        label='Validation', color='skyblue', alpha=0.8)\n",
    "ax2.set_ylabel('Anomaly Rate (%)', fontweight='bold')\n",
    "ax2.set_title('Anomaly Rate: Train vs Validation', fontweight='bold', fontsize=12)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.axhline(y=CONTAMINATION_RATE*100, color='red', linestyle='--', \n",
    "            label=f'Expected ({CONTAMINATION_RATE*100}%)', linewidth=2)\n",
    "\n",
    "# 3. Rate consistency\n",
    "ax3 = axes[1, 0]\n",
    "bars3 = ax3.barh(comparison_df['Model'], comparison_df['Rate Consistency'], color='mediumseagreen')\n",
    "ax3.set_xlabel('Rate Consistency (lower = better)', fontweight='bold')\n",
    "ax3.set_title('Train-Val Consistency', fontweight='bold', fontsize=12)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, bar in enumerate(bars3):\n",
    "    width = bar.get_width()\n",
    "    ax3.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.2f}%', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 4. Total anomalies detected\n",
    "ax4 = axes[1, 1]\n",
    "x_pos = np.arange(len(comparison_df))\n",
    "ax4.bar(x_pos - width/2, comparison_df['Train Anomalies'], width, \n",
    "        label='Train', color='coral', alpha=0.8)\n",
    "ax4.bar(x_pos + width/2, comparison_df['Val Anomalies'], width, \n",
    "        label='Validation', color='skyblue', alpha=0.8)\n",
    "ax4.set_ylabel('Number of Anomalies', fontweight='bold')\n",
    "ax4.set_title('Total Anomalies Detected', fontweight='bold', fontsize=12)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Anomaly Detection Model Benchmarking', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHARTS_DIR / \"03_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Saved comparison chart to: {CHARTS_DIR / '03_model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Select Best Model & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on rate consistency and expected contamination\n",
    "best_model_idx = comparison_df['Rate Consistency'].idxmin()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_result = all_results[best_model_idx]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BEST MODEL SELECTED: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nSelection Criteria:\")\n",
    "print(f\"  - Lowest train-val rate consistency: {comparison_df.loc[best_model_idx, 'Rate Consistency']:.2f}%\")\n",
    "print(f\"  - Train anomaly rate: {best_result['train_anomaly_rate']:.2f}%\")\n",
    "print(f\"  - Val anomaly rate: {best_result['val_anomaly_rate']:.2f}%\")\n",
    "print(f\"  - Training time: {best_result['train_time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL EVALUATION ON TEST SET\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "best_model = best_result['model']\n",
    "\n",
    "# Predict on test set\n",
    "if best_model_name in ['Local Outlier Factor', 'DBSCAN']:\n",
    "    # These models require fit_predict\n",
    "    y_test_pred = best_model.fit_predict(X_test_scaled)\n",
    "else:\n",
    "    y_test_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert if necessary\n",
    "if set(np.unique(y_test_pred)).issubset({0, 1}):\n",
    "    y_test_pred = np.where(y_test_pred == 0, -1, 1)\n",
    "\n",
    "test_anomalies = (y_test_pred == -1).sum()\n",
    "test_anomaly_rate = test_anomalies / len(y_test_pred) * 100\n",
    "\n",
    "print(f\"\\nTest Set Results ({best_model_name}):\")\n",
    "print(f\"  Total samples : {len(y_test_pred):,}\")\n",
    "print(f\"  Normal        : {(y_test_pred == 1).sum():,} ({100 - test_anomaly_rate:.2f}%)\")\n",
    "print(f\"  Anomalies     : {test_anomalies:,} ({test_anomaly_rate:.2f}%)\")\n",
    "\n",
    "# Get anomaly scores if available\n",
    "test_scores = None\n",
    "if hasattr(best_model, 'decision_function'):\n",
    "    try:\n",
    "        test_scores = best_model.decision_function(X_test_scaled)\n",
    "        print(f\"\\nAnomaly Scores:\")\n",
    "        print(f\"  Min  : {test_scores.min():.4f}\")\n",
    "        print(f\"  Max  : {test_scores.max():.4f}\")\n",
    "        print(f\"  Mean : {test_scores.mean():.4f}\")\n",
    "        print(f\"  Std  : {test_scores.std():.4f}\")\n",
    "    except:\n",
    "        pass\n",
    "elif hasattr(best_model, 'score_samples'):\n",
    "    try:\n",
    "        test_scores = best_model.score_samples(X_test_scaled)\n",
    "        print(f\"\\nAnomaly Scores:\")\n",
    "        print(f\"  Min  : {test_scores.min():.4f}\")\n",
    "        print(f\"  Max  : {test_scores.max():.4f}\")\n",
    "        print(f\"  Mean : {test_scores.mean():.4f}\")\n",
    "        print(f\"  Std  : {test_scores.std():.4f}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Comprehensive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataset predictions using best model\n",
    "print(\"Generating predictions on full dataset...\")\n",
    "\n",
    "# Scale full dataset\n",
    "X_full_scaled = scaler.transform(X)\n",
    "\n",
    "# Predict\n",
    "if best_model_name in ['Local Outlier Factor', 'DBSCAN']:\n",
    "    y_full_pred = best_model.fit_predict(X_full_scaled)\n",
    "else:\n",
    "    # Re-fit on full dataset for production model\n",
    "    best_model.fit(X_full_scaled)\n",
    "    y_full_pred = best_model.predict(X_full_scaled)\n",
    "\n",
    "# Convert if necessary\n",
    "if set(np.unique(y_full_pred)).issubset({0, 1}):\n",
    "    y_full_pred = np.where(y_full_pred == 0, -1, 1)\n",
    "\n",
    "# Get scores\n",
    "full_scores = None\n",
    "if hasattr(best_model, 'decision_function'):\n",
    "    try:\n",
    "        full_scores = best_model.decision_function(X_full_scaled)\n",
    "    except:\n",
    "        full_scores = np.zeros(len(y_full_pred))  # Placeholder\n",
    "elif hasattr(best_model, 'score_samples'):\n",
    "    try:\n",
    "        full_scores = best_model.score_samples(X_full_scaled)\n",
    "    except:\n",
    "        full_scores = np.zeros(len(y_full_pred))  # Placeholder\n",
    "else:\n",
    "    full_scores = np.zeros(len(y_full_pred))  # Placeholder\n",
    "\n",
    "print(f\"‚úì Generated {len(y_full_pred):,} predictions\")\n",
    "print(f\"  Anomalies detected: {(y_full_pred == -1).sum():,} ({(y_full_pred == -1).sum()/len(y_full_pred)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = df_encoded.copy()\n",
    "results_df['prediction'] = y_full_pred\n",
    "results_df['anomaly_score'] = full_scores\n",
    "results_df['is_anomaly'] = (y_full_pred == -1).astype(int)\n",
    "\n",
    "# Save results\n",
    "results_csv_path = OUTPUTS_DIR / \"04_anomaly_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"‚úì Saved results to: {results_csv_path}\")\n",
    "\n",
    "# Save as JSON\n",
    "results_json_path = OUTPUTS_DIR / \"04_anomaly_results.json\"\n",
    "results_df.to_json(results_json_path, orient='records', indent=2)\n",
    "print(f\"‚úì Saved results to: {results_json_path}\")\n",
    "\n",
    "print(f\"\\nResults DataFrame shape: {results_df.shape}\")\n",
    "print(f\"Columns: {results_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly distribution by location\n",
    "# Reverse one-hot encoding to get location names\n",
    "location_cols = [col for col in results_df.columns if col.startswith('loc_')]\n",
    "results_df['location_decoded'] = results_df[location_cols].idxmax(axis=1).str.replace('loc_', '')\n",
    "\n",
    "anomaly_by_location = results_df.groupby('location_decoded')['is_anomaly'].agg(['sum', 'count', 'mean'])\n",
    "anomaly_by_location.columns = ['Anomalies', 'Total', 'Anomaly_Rate']\n",
    "anomaly_by_location['Anomaly_Rate'] = anomaly_by_location['Anomaly_Rate'] * 100\n",
    "anomaly_by_location['Normal'] = anomaly_by_location['Total'] - anomaly_by_location['Anomalies']\n",
    "\n",
    "print(\"\\nüìç ANOMALY DISTRIBUTION BY LOCATION\")\n",
    "print(\"=\"*70)\n",
    "print(anomaly_by_location)\n",
    "\n",
    "# Save\n",
    "anomaly_by_location.to_csv(OUTPUTS_DIR / \"05_anomalies_by_location.csv\")\n",
    "print(f\"\\n‚úì Saved location analysis to: {OUTPUTS_DIR / '05_anomalies_by_location.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis\n",
    "anomaly_by_hour = results_df.groupby('hour')['is_anomaly'].agg(['sum', 'count', 'mean'])\n",
    "anomaly_by_hour.columns = ['Anomalies', 'Total', 'Anomaly_Rate']\n",
    "anomaly_by_hour['Anomaly_Rate'] = anomaly_by_hour['Anomaly_Rate'] * 100\n",
    "\n",
    "print(\"\\n‚è∞ ANOMALY DISTRIBUTION BY HOUR\")\n",
    "print(\"=\"*70)\n",
    "print(anomaly_by_hour)\n",
    "\n",
    "# Save\n",
    "anomaly_by_hour.to_csv(OUTPUTS_DIR / \"06_anomalies_by_hour.csv\")\n",
    "print(f\"\\n‚úì Saved hour analysis to: {OUTPUTS_DIR / '06_anomalies_by_hour.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Anomaly score distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Score distribution\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(full_scores[y_full_pred == 1], bins=50, alpha=0.7, label='Normal', color='skyblue', edgecolor='black')\n",
    "ax1.hist(full_scores[y_full_pred == -1], bins=50, alpha=0.7, label='Anomaly', color='red', edgecolor='black')\n",
    "ax1.set_xlabel('Anomaly Score', fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontweight='bold')\n",
    "ax1.set_title('Anomaly Score Distribution', fontweight='bold', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# 2. Anomalies by location\n",
    "ax2 = axes[0, 1]\n",
    "anomaly_by_location[['Normal', 'Anomalies']].plot(kind='bar', stacked=True, ax=ax2, \n",
    "                                                    color=['skyblue', 'red'], alpha=0.8)\n",
    "ax2.set_ylabel('Count', fontweight='bold')\n",
    "ax2.set_title('Anomalies by Location', fontweight='bold', fontsize=12)\n",
    "ax2.set_xlabel('Location', fontweight='bold')\n",
    "ax2.legend(['Normal', 'Anomalies'])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Anomalies by hour\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(anomaly_by_hour.index, anomaly_by_hour['Anomaly_Rate'], marker='o', \n",
    "         linewidth=2, markersize=8, color='darkred')\n",
    "ax3.set_xlabel('Hour of Day', fontweight='bold')\n",
    "ax3.set_ylabel('Anomaly Rate (%)', fontweight='bold')\n",
    "ax3.set_title('Anomaly Rate by Hour of Day', fontweight='bold', fontsize=12)\n",
    "ax3.grid(alpha=0.3)\n",
    "ax3.set_xticks(range(0, 24, 2))\n",
    "\n",
    "# 4. Feature comparison (anomaly vs normal)\n",
    "ax4 = axes[1, 1]\n",
    "normal_data = results_df[results_df['prediction'] == 1][SENSOR_FEATURES[:3]].mean()\n",
    "anomaly_data = results_df[results_df['prediction'] == -1][SENSOR_FEATURES[:3]].mean()\n",
    "\n",
    "x_pos = np.arange(len(SENSOR_FEATURES[:3]))\n",
    "width = 0.35\n",
    "ax4.bar(x_pos - width/2, normal_data, width, label='Normal', color='skyblue', alpha=0.8)\n",
    "ax4.bar(x_pos + width/2, anomaly_data, width, label='Anomaly', color='red', alpha=0.8)\n",
    "ax4.set_ylabel('Mean Value', fontweight='bold')\n",
    "ax4.set_title('Feature Comparison: Normal vs Anomaly', fontweight='bold', fontsize=12)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels([f.replace('_', ' ').title() for f in SENSOR_FEATURES[:3]], \n",
    "                     rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Anomaly Detection Results - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig(CHARTS_DIR / \"04_anomaly_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"‚úì Saved analysis charts to: {CHARTS_DIR / '04_anomaly_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = ARTIFACTS_DIR / f\"best_model_{best_model_name.lower().replace(' ', '_').replace('-', '_')}.joblib\"\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úì Saved best model to: {best_model_path}\")\n",
    "\n",
    "# Save feature column names\n",
    "feature_config = {\n",
    "    'feature_columns': feature_columns,\n",
    "    'sensor_features': SENSOR_FEATURES,\n",
    "    'temporal_features': TEMPORAL_FEATURES,\n",
    "    'n_features': len(feature_columns),\n",
    "    'best_model_name': best_model_name,\n",
    "    'contamination_rate': CONTAMINATION_RATE,\n",
    "    'random_seed': RANDOM_SEED,\n",
    "}\n",
    "\n",
    "feature_config_path = ARTIFACTS_DIR / \"feature_config.json\"\n",
    "with open(feature_config_path, 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "print(f\"‚úì Saved feature config to: {feature_config_path}\")\n",
    "\n",
    "# Create production pipeline\n",
    "production_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "pipeline_path = ARTIFACTS_DIR / \"production_pipeline.joblib\"\n",
    "joblib.dump(production_pipeline, pipeline_path)\n",
    "print(f\"‚úì Saved production pipeline to: {pipeline_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL ARTIFACTS SAVED SUCCESSFULLY\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_report = f\"\"\"\n",
    "{'='*80}\n",
    "ANOMALY DETECTION PIPELINE - SUMMARY REPORT\n",
    "{'='*80}\n",
    "\n",
    "PROJECT: Natural Gas Pipeline Operations Monitoring\n",
    "DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "DATA SUMMARY\n",
    "{'='*80}\n",
    "Total Records (Raw)      : {len(df_raw):,}\n",
    "Records After Cleaning   : {len(df_clean):,}\n",
    "Records Removed          : {len(df_raw) - len(df_clean):,} ({(len(df_raw) - len(df_clean))/len(df_raw)*100:.2f}%)\n",
    "Locations                : {', '.join(df_clean['location'].unique())}\n",
    "Time Range               : {df_clean['timestamp'].min()} to {df_clean['timestamp'].max()}\n",
    "Duration                 : {(df_clean['timestamp'].max() - df_clean['timestamp'].min()).days} days\n",
    "\n",
    "{'='*80}\n",
    "FEATURE ENGINEERING\n",
    "{'='*80}\n",
    "Sensor Features          : {len(SENSOR_FEATURES)}\n",
    "Temporal Features        : {len(TEMPORAL_FEATURES)}\n",
    "Location Features        : {len([col for col in feature_columns if col.startswith('loc_')])}\n",
    "Total Features           : {len(feature_columns)}\n",
    "\n",
    "{'='*80}\n",
    "MODEL TRAINING\n",
    "{'='*80}\n",
    "Models Evaluated         : {len(all_results)}\n",
    "Best Model               : {best_model_name}\n",
    "Selection Criterion      : Lowest train-val rate consistency\n",
    "\n",
    "Train Set Size           : {X_train.shape[0]:,} ({X_train.shape[0]/X.shape[0]*100:.1f}%)\n",
    "Validation Set Size      : {X_val.shape[0]:,} ({X_val.shape[0]/X.shape[0]*100:.1f}%)\n",
    "Test Set Size            : {X_test.shape[0]:,} ({X_test.shape[0]/X.shape[0]*100:.1f}%)\n",
    "\n",
    "{'='*80}\n",
    "BEST MODEL PERFORMANCE ({best_model_name})\n",
    "{'='*80}\n",
    "Training Time            : {best_result['train_time']:.3f}s\n",
    "Train Anomaly Rate       : {best_result['train_anomaly_rate']:.2f}%\n",
    "Val Anomaly Rate         : {best_result['val_anomaly_rate']:.2f}%\n",
    "Test Anomaly Rate        : {test_anomaly_rate:.2f}%\n",
    "Rate Consistency         : {comparison_df.loc[best_model_idx, 'Rate Consistency']:.2f}%\n",
    "\n",
    "{'='*80}\n",
    "FINAL RESULTS (Full Dataset)\n",
    "{'='*80}\n",
    "Total Predictions        : {len(y_full_pred):,}\n",
    "Normal Observations      : {(y_full_pred == 1).sum():,} ({(y_full_pred == 1).sum()/len(y_full_pred)*100:.2f}%)\n",
    "Anomalies Detected       : {(y_full_pred == -1).sum():,} ({(y_full_pred == -1).sum()/len(y_full_pred)*100:.2f}%)\n",
    "\n",
    "Top Anomalous Location   : {anomaly_by_location['Anomaly_Rate'].idxmax()}\n",
    "Location Anomaly Rate    : {anomaly_by_location['Anomaly_Rate'].max():.2f}%\n",
    "\n",
    "Peak Anomaly Hour        : {anomaly_by_hour['Anomaly_Rate'].idxmax()}:00\n",
    "Peak Hour Anomaly Rate   : {anomaly_by_hour['Anomaly_Rate'].max():.2f}%\n",
    "\n",
    "{'='*80}\n",
    "OUTPUT FILES\n",
    "{'='*80}\n",
    "\n",
    "CHARTS (PNG):\n",
    "  ‚úì {CHARTS_DIR / '01_correlation_matrix.png'}\n",
    "  ‚úì {CHARTS_DIR / '02_feature_distributions.png'}\n",
    "  ‚úì {CHARTS_DIR / '03_model_comparison.png'}\n",
    "  ‚úì {CHARTS_DIR / '04_anomaly_analysis.png'}\n",
    "\n",
    "OUTPUTS (CSV/JSON):\n",
    "  ‚úì {OUTPUTS_DIR / '01_data_quality_report_raw.csv'}\n",
    "  ‚úì {OUTPUTS_DIR / '02_descriptive_statistics.csv'}\n",
    "  ‚úì {OUTPUTS_DIR / '03_model_comparison.csv'}\n",
    "  ‚úì {OUTPUTS_DIR / '04_anomaly_results.csv'}\n",
    "  ‚úì {OUTPUTS_DIR / '04_anomaly_results.json'}\n",
    "  ‚úì {OUTPUTS_DIR / '05_anomalies_by_location.csv'}\n",
    "  ‚úì {OUTPUTS_DIR / '06_anomalies_by_hour.csv'}\n",
    "\n",
    "ARTIFACTS (Model Files):\n",
    "  ‚úì {ARTIFACTS_DIR / 'scaler.joblib'}\n",
    "  ‚úì {best_model_path}\n",
    "  ‚úì {ARTIFACTS_DIR / 'feature_config.json'}\n",
    "  ‚úì {ARTIFACTS_DIR / 'production_pipeline.joblib'}\n",
    "\n",
    "{'='*80}\n",
    "CONFIGURATION\n",
    "{'='*80}\n",
    "Random Seed              : {RANDOM_SEED}\n",
    "Expected Contamination   : {CONTAMINATION_RATE * 100}%\n",
    "Test Size                : {TEST_SIZE * 100}%\n",
    "Validation Size          : {VAL_SIZE * 100}%\n",
    "\n",
    "{'='*80}\n",
    "REPRODUCIBILITY\n",
    "{'='*80}\n",
    "All random seeds are fixed at {RANDOM_SEED} for full reproducibility.\n",
    "To reproduce results:\n",
    "  1. Load data from /data directory\n",
    "  2. Run this notebook from top to bottom\n",
    "  3. All outputs will be regenerated identically\n",
    "\n",
    "{'='*80}\n",
    "END OF REPORT\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "summary_path = OUTPUTS_DIR / \"00_SUMMARY_REPORT.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "print(f\"\\n‚úì Saved summary report to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Production Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to use the production pipeline for inference\n",
    "print(\"=\"*80)\n",
    "print(\"PRODUCTION INFERENCE EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load production pipeline\n",
    "loaded_pipeline = joblib.load(ARTIFACTS_DIR / \"production_pipeline.joblib\")\n",
    "loaded_config = json.load(open(ARTIFACTS_DIR / \"feature_config.json\"))\n",
    "\n",
    "print(\"\\n‚úì Loaded production pipeline and config\")\n",
    "print(f\"  Model: {loaded_config['best_model_name']}\")\n",
    "print(f\"  Features: {loaded_config['n_features']}\")\n",
    "\n",
    "# Simulate new data (first 5 rows from test set)\n",
    "new_data = X_test[:5]\n",
    "print(f\"\\nSimulating {len(new_data)} new samples...\")\n",
    "\n",
    "# Make predictions\n",
    "if loaded_config['best_model_name'] in ['Local Outlier Factor', 'DBSCAN']:\n",
    "    # These models need special handling\n",
    "    print(\"Note: LOF and DBSCAN require retraining for new predictions.\")\n",
    "    predictions = loaded_pipeline.predict(new_data)\n",
    "else:\n",
    "    predictions = loaded_pipeline.predict(new_data)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPredictions:\")\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    label = \"ANOMALY\" if pred == -1 else \"NORMAL\"\n",
    "    print(f\"  Sample {i}: {label}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
